{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea1414-2cb1-4535-9d04-f8b98ffbd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe9f64-f475-4312-81b8-634cac4a2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfe626-f20b-4744-96ca-8dfe9dd0f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e082a-6672-4906-88ba-7b2fe611936e",
   "metadata": {},
   "source": [
    "While scikit-learn's CountVectorizer is the standard tool for converting text into a Bag of N-grams matrix, spaCy is often preferred for the preprocessing stage because it provides deeper linguistic intelligence that improves the quality of those n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f90d40-0847-4f14-852f-7c8f5bcc34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963850b4-51db-4a67-8208-e4a3d6a60c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dc553-1ae9-4d9f-ba23-4348c6990bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"Loki is eating pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a28a1-5816-4896-a614-4460a858c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"Thor ate pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0ba23-3c15-457b-b8dd-24d05aa1fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea70a9-e0d2-4fd7-b010-20a17dc44639",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_processed = [preprocess(text) for text in corpus]\n",
    "corpus_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e76022-6c30-4195-9542-63b73b9acb20",
   "metadata": {},
   "source": [
    "By using spaCy for the heavy lifting and CountVectorizer for the final matrix, you get the best of both worlds: linguistic accuracy and mathematical efficiency.\n",
    "\n",
    "The Workflow:\n",
    "\n",
    "* Feed text to spaCy: Let it analyze the grammar and context.\n",
    "* Filter & Transform:\n",
    "* * Check token.is_stop to kill noise.\n",
    "* * Check token.is_punct to remove punctuation.\n",
    "* * Grab token.lemma_ to get the root word.\n",
    "   Feed to CountVectorizer: Pass this \"cleaned\" list of lemmas into the vectorizer.\n",
    "\n",
    "Why this specific order?\n",
    "\n",
    "If you use CountVectorizer's built-in stop_words='english', it tries to remove words before they are lemmatized.\n",
    "* The Problem: \"Organizing\" might be in a stop-word list, but its lemma \"organize\" might not be.\n",
    "* The Result: You end up with \"messy\" data where some versions of a word are removed and others aren't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35bd1f-17b1-4c37-a0f4-4477c75fc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer(ngram_range=(1,2)) \n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b26521-3b35-462e-b5d5-bb2b34989686",
   "metadata": {},
   "source": [
    "\n",
    "Now generate bag of n gram vector for few sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cec099-f741-4b95-8101-acf10ad397bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transform(['Thor eat pizza']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef2cfc-a6b3-49ec-9bd1-d7c663ec4c02",
   "metadata": {},
   "source": [
    "Let's take a document that has out of vocabulary (OOV) term and see how bag of ngram generates vector out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aea291-8f7a-49ec-9fa7-01067fdbdf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transform(['Tanu eat snacks']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b8b3b-2200-42e1-a705-ef6544312699",
   "metadata": {},
   "source": [
    "* News Category Classification Problem : \n",
    "Here we want to do a news category classification. We will use bag of n-grams and traing a machine learning model that can categorize any news into one of the following categories,\n",
    "\n",
    "* BUSINESS\n",
    "* SPORTS\n",
    "* CRIME\n",
    "* SCIENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00099d58-5a7c-497a-87dd-eaacfa5e6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('news_dataset.json')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0425e-cd2e-4741-b802-ff465312f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087c224-1735-436b-adfd-779e9f2b522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 1381\n",
    "\n",
    "df_business = df[df.category=='BUSINESS'].sample(min_samples,random_state=2022)\n",
    "df_sports = df[df.category=='SPORTS'].sample(min_samples,random_state=2022)\n",
    "df_crime = df[df.category=='CRIME'].sample(min_samples,random_state=2022)\n",
    "df_science = df[df.category=='SCIENCE'].sample(min_samples,random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae37896-74c8-4d4c-8fb2-b6138583dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df_business,df_sports,df_crime,df_science],axis=0)\n",
    "df_balanced.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc8c45-15af-488c-aaab-da5bb4ad8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cfebd-d78b-4104-a56e-c7dbe9fa297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6154ba4-1a3d-48e6-9526-4969c3bee0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {'BUSINESS':0,'SPORTS':1,'CRIME':2,'SCIENCE':3}\n",
    "\n",
    "df_balanced['category_num'] = df_balanced['category'].map(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0840d7-4303-4566-ae6e-2fb5a8c5a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a144aa-86bb-4d5b-a1b3-81a91b9c06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['preprocessed_txt'] = df_balanced['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062ddaf-dc2d-464e-8519-77b83d183b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975a21d-060e-4ef3-8843-ff2dcedefb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89740a1-eeac-4f8e-b31c-95f50662025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_balanced.preprocessed_txt,df_balanced.category_num,test_size=0.25,random_state=2022,stratify=df_balanced.category_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a09e47-a9f8-4aa2-b8cd-8f47e2ca0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c598f-3677-4965-8406-53d51f18ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42e221-78e8-4659-bd1d-54e4a525932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0a121-324e-4f59-8fd4-661ca0f61bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = Pipeline([\n",
    "    ('vectorizer',CountVectorizer(ngram_range=(1,2))),\n",
    "    ('nb',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bb53c-41cc-4f0c-8265-346bc02e171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e326b5d-b01a-4bb2-ac0b-672538b3e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7440a7-c037-4ab8-bb2a-d117d43028b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c8579-8bc9-45e4-a16c-f50d84452f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca93336-deeb-4d97-bf6b-83028f378adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm,annot=True,fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
